\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amssymb}

\begin{document}
    \section{Image Segmentation}

    \subsection{Problem Statement}
    In this section, we focus on using probabilistic graphical models to segment an image into 2: foreground and background.
    Each pixel belongs to one of the segments and the assignment of each pixel can be modelled as a categorical latent variable (Z),
    while the pixel value is the observed variable (X). The observed variables can be modelled as a multivariate normal distribution.
    The observations are 3-dimensional and each pixel can either be represented in the RGB space or the Lab space.

    \subsection{Approach}
    This is a clustering problem with 2 clusters, each represented as a Gaussian. The input is an image with $N$ pixels and
    each pixel is represented as a 3-dimensional random variable $\bm{x}_n$ where, $n=1,...,N$. $\bm{z}_n$ is a latent
    random variable that denotes the segment each observation belongs to and follows a categorical distribution that can be represented as:

    \begin{equation}
        p(Z) = \prod_{k=1}^{2} \pi_k^{z_k}
    \end{equation}
    \noindent
    where the parameter $\pi = [\pi_1, \pi_2]$ represent the probability of Z taking each of the 2 states. This paramater also represents
    mixing coefficients of each cluster's Gaussian. The Gaussian mixture model is given by:

    \begin{equation}
        p(\bm{x}) = \sum_{k=1}^{2} \pi_k \mathcal{N} \left(\bm{x}|\bm{\mu}_k,\bm{\Sigma}_k\right)
    \end{equation}
    \noindent
    The responsibility that each segment $k$ takes for explaining each observation $\bm{x}_n$ is given by $\gamma(\bm{Z})$
    which is an N x 2 table. We aim to find the parameters, $\theta$, that maximize the log-likelihood of the model, which is given by:

    \begin{equation}
        ln p(\bm{x}_1,...,\bm{x}_N|\theta) = \sum_{n=1}^{N}ln\sum_{\bm{z}_n}p(\bm{x}_n, \bm{z}_n|\theta), \theta = \left\{\pi_1,\pi_2,\mu_1,\mu_2,\Sigma_1,\Sigma_2 \right\}
    \end{equation}

    \noindent
    Since we cannot obtain a closed form solution of the unknown parameters by setting the derivatives to 0, we initialize the parameters to sensible initial values and iteratively calculate new responsibilities and new parameters until the log-likelihood converges.
    To obtain sensible initializations, we first perform K-means clustering to estimate $\pi_k$, $\mu_k$ and $\Sigma_k$ for each cluster. Then we proceeded to the expectation step where we calculated the responsibilities as:

    \begin{equation}
        \gamma(z_{nk}) = \frac{\pi_k \mathcal{N} \left(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k\right)}{\sum_{j=1}^{2} \pi_j \mathcal{N} \left(\bm{x}_n|\bm{\mu}_j,\bm{\Sigma}_j\right)}
    \end{equation}

    \noindent
    Next, in the maximization step, we re-estimate the parameters using the current responsibilities as such:

    \begin{equation}
        \bm{\mu}_k^{\mathnormal{new}} = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk})\bm{x}_n
    \end{equation}

    \begin{equation}
        \bm{\Sigma}_k^{\mathnormal{new}} = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk})\left(\bm{x}_n-\bm{\mu}_k^{\mathnormal{new}}\right)\left(\bm{x}_n-\bm{\mu}_k^{\mathnormal{new}}\right)^T
    \end{equation}

    \begin{equation}
        \bm{\pi}_k^{\mathnormal{new}} = \frac{N_k}{N}
    \end{equation}

    \noindent
    where,

    \begin{equation}
        N_k = \sum_{n=1}^N\gamma(z_{nk})
    \end{equation}

    \noindent
    After every iteration, we calculate the log-likelihood as specified in Equation 3.  The difference in the increase in log-likelihood after every iteration was computed. If the difference was less than $1\times10^{-6}$, the iteration was stopped and the final responsibilities were calculated.
    The segment that had the highest responsibility for each observation was assigned as the label. \\

    \subsection{Results and Discussion}
    Using EM algorithm, we successfully segmented all 4 test images into distinct foreground and background segments, with minimal noise. We experimented with using 3 features (the L, a, b channels) and 5 features (x-coordinate, y-coordinate and the L, a, b channels). We found that the results were better when 5 features were used. The input data was scaled along each feature independently using their respective means and standard deviations. We did not notice any difference in result between using the raw data and scaled data. However, the runtime was faster when scaled data was used.

\end{document}